10주차 정리노트 -최정환, 김성영

이번 10주차 기계학습 강의에서는 ‘차원축소’에 대해서 학습하였다.
차원축소란 학습 불가능한 문제들을 특성의 숫자를 줄여 학습이 가능하게 만드는 기법이다. 
차원축소는 특성수가 줄어들기 때문에 훈련속도가 빨라지지만 그만큼 데이터가 유실되는 것이기에 성능자체는 저하될 수 있는 가능성을 가지고 있다.
이러한 차원축소를 할 때는 대표적으로 투영과 매니폴드 학습, 두가지 방식이 있다. 

첫번째로 투영으로 차원축소를 하는 방식은 차원을 한단계 낮추어 그 차원에 최대한 많은 데이터들을 포함시키게 하는 것이다. 
그 후 계속해서 데이터셋들을 포함한 차원을 수집하여 분석한다. 

이 투영기법은 PCA와 커널PCA, 두가지 알고리즘을 사용할 수 있다.
PCA는 데이터가 가장 많이 포함된 초평면을 찾고 이것을 주성분이라 정의내린뒤 이 부분과 수직으로 연결된 추가의 주성분을 찾는 방식으로 진행된다. 
이 때 주성분을 찾아 투영할때에는 반드시 분산이 최대로 보존되는 축을 선택하여 만들어야한다. 
그렇게 해야만 차원축소에서 우려하는 점인 정보손실을 최대한 줄일 수 있기 때문이다.

이때 차원의 개수를 적절하게 조절하는 것도 중요하다. 개수를 조절하는 기준에 필요한 것은 바로 분산 비율의 합이다. 
충분한 분산의 기준은 95%로 분산 비율의 합이 이를 충족하는 만큼 차원의 수를 선택하여야한다. 
최소한의 자원으로 최대한의 효율을 낼 수 있기 때문이다.
이 중 사이킷런에서 진행되었던 것 중 PCA 객체를 이용한 것과 SVD를 이용한 방법이 비교하는 과정이 있었는데 이 중에 의문이 생겨 이야기를 나누었다. 
두개의 방법에서 나온 값자체는 같은데 부호가 바뀐것이다. 
이에 같은 초평면이지만 뒤집혀서 나온것일뿐 부호는 그렇게 크게 중요하지 않으며 여기서 중점적으로 바라보아야할 것은 ‘값이 같다는’부분이라는것을 깨달았다.

이후 inverse_transform()함수를 사용하여 차원축소하였던 훈련세트를 다시 원래크기로 되돌리는 실습을 하였다. 
확실히 95%의 분산을 유지하였기때문인지 복원되었던 데이터들의 정보손실을 크지않았다.
여기서 조금 논외의 이야기이지만 이 개념을 반대로 사용하여 데이터들을 복원할 수 없게 만들 수 있지 않을까라는 생각들을 해보았다. 
하지만 그건 조금 힘들것이다라는 생각에 도달했다. 

95%의 분산을 유지하는 데이터셋을 복원할 수 없게 만들려면 데이터셋자체에 불순물을 넣거나 복원함수에 손을 댄다는 생각을 해보았지만 어느쪽이던 누군가의 복원을 막기위해서 실행하려면 부단한 노력이 필요할 듯 싶었다.
두번째로 매니폴드 학습은 스위스 롤같이 투영으로는 도저히 힘든 데이터들을 펼침으로서 해결할 수 있게 해준다. 

매이폴드 학습은 LocallyLinearEmbedding라는 코드를 사용하여 스위스롤 같은 데이터를 펼쳐주며 이는 LLE라는 축소 기법을 사용한 것이다.
