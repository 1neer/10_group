기계학습 4주차 정리노트
-김성영 최정환

이번 강의에서는 주로 선형회귀 모델이 어떻게 학습을 진행하는지에 대해서 배우도 토론하였다.


먼저 주요한 내용으로는 경사 하강법이 있었다. 경사 하강법이란 비용 함수를 최소화 하기 위해 반복해서 파라미터를 조정해 나가는데,
배치 경사 하강법, 확률적 경사 하강법, 미니배치 경사 하강법이 있다.
그중에서 우리가 토론한 부분은 확률적 경사 하강법이, 비용함수가 매우 불규칙할때, 알고리즘이 지역 최솟값을 건너뛰어,
전역 최솟값을 찾을 가능성이 높다고 나와 있었는데 왜 이러한 결과가 도출이 될까에 대해서 토론을 했었다.
그리고 우리는 그 이유를 확률적 경사 하강법의 무작위성 때문이라고 결론 내렸다.
확률적 경사 하강법이란 매 스텝에서 한 개의 샘플을 랜덤하게 선택하고, 그 하나에 대한 그레이디언트를 계산하는 방식인데,
일반적인 배치 경사 하강법의 경우 모든 데이터셋에 대해 한번에 그레이디언트를 계산하기 때문에,
지역 최솟값에 도달할 경우 그 곳을 벗어나기 힘들다. 
하지만, 확률적 경사하강법 같은 경우
샘플을 랜덤하게 선택한 후 하나하나 그레이디언트를 게산하기 때문에 불안정한 면이 있지만, 반대로 생각해보면 이러한 점이
오히려 지역 최솟값을 벗어나기 쉽게 해줄 수 있을 것이라는 결론을 내렸다.

다음으로는 데이터셋 갯수가 1000개이고, 미니배치는 50, epoc가 100번일때 가중치의 업데이트는 몇 번 이루어 질까에 대해서 토론했다.
먼저, 데이터셋의 갯수가 1000개이고, 미니배치는 50이니, 샘플의 수가 총20개 존재한다. 결국 20epoc를 돌때마다 한번의 가중치 업데이트가 이루어 지는 것이니,
답은 총 4번의 가중치 업데이트가 이루어 진다고 결론을 내렸다.

마지막으로는 규제가 있는 선형모델 중에서도 로지스틱 회귀에 대해 이야기 했다.
먼저 로지스틱 회귀는 우리가 그전까지 배웠던 선형 회귀들과는 다르게, 확률을 예측해주는 모델로써 사용되었다.
이는 독립변수의 선형 결합을 이용하여 사건의 발생 가능성을 예측하는 데 사용되는 통계 기법이다.
로지스틱 회귀는 일반적인 선형 회귀처럼 바로 결과를 출력하는 것이 하닌, 결괏값의 로지스틱(s자 형태의 시그모이드 함수)을 출력해준다.
또한, 로지스틱 회귀는 일반적으로 이진분류 문제에서 많이 활용하는데, 이진분류가 아닌 다중 클래스 분류를 위해서는 소프트맥스 회귀,
또는 다항 로지스틱 회귀로 부르는 모델을 사용해야 한다는 것도 배웠다.
