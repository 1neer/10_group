9주차 정리노트
-최정환, 김성영

오늘은 팀원이 중간에 조퇴를 했기 때문에 토론한 내용보다는 배웠던 내용을 정리하는 쪽으로 노트를 작성하였다.

앙상블이란 여러개의 예특기로 이루어진 그룹이고, 예측기 여러개의 결과를 종합하여 예측값을 지정하는 학습을 앙상블 학습이라 부른다.
그중에는 동일한 훈련세트에 대해 여러종류의 분류기를 이용한 앙상블 학습 정용 후 직접 또는 간접 투표를 통해 예측값을 결정하는 투표기반 분류기가 있다.

먼저 투표기반 분류기는 큰수의 법칙에 기반하여 만들어진 분류기이고, 직접투표방식과 간접투표방식이 있는데, 직접투표는 앙상블에 포함된 예측기들의 예측값들을 다수결 투표로
결정하는 방식이고, 간접투표는 앙상블에 포함된 예츠기들의 예측한 확률갓들의 평균값으로 예측값을 결정한다. 이 때, 전제는 모든 예측기가 predict_proba()메서드와 같은
확률 예측 기능을 지원해야한다.

코드상에서 voting하이퍼파라미터를 hard로 설정하면 직접, soft로 설정하면 간접투표방식이 된다. 하지만 실습을 할때 voting을 soft로 설정하고 간접투표방식으로 예측을 진행하였는데,
오류가 발생하였다. 이것이 왜 이런가 보니 predict_proba()와 같은 확률 예측기능을 로지스틱 회귀와 랜덤포레스트는 지원을 하는데, SVC모델은 이를 지원을 하지 않아서,
SVC모델의 probability하이퍼파라미터를 True로 설정해주어야 정상적으로 코드가 동작하는 것을 확인하였다.

동일한 예측기를 훈련세트의 다양한 부분집합을 대상으로 학습시키는 방식을 배깅or 페이스팅이라 하는데, 부분집합을 임의로 선택할 때 중복 허용 여부에 따라 앙상블 학습 방식이 달라지는데,\
중복을 허용하는 샘플링 방식은 배깅이고, 중복을 미허용하는 샘플링 방식은 페이스팅이라고 한다.

그중 배깅을 사용하면 어떤 샘플은 한 예측기를 위해 여러번 샘플링 되고, 어떤것은 전혀 사용되지 않는데 이를 oob샘플이라 하고 이는 선택되지 않은 훈련 샘플을 뜻한다.
이 oob샘플을 활용해서 앙상블 학습에 사용된 개별 예측기의 성능을 평가 가능한데, 이는 우리가 다른 모델에서도 흔히 사용하는 검증세트와 그 결이 비슷하다.
검증세트 또한 원래 테스트세트에서 테스트세트와 검증세트로 나누는데, 검증세트는 최종적으로 학습에 사용된 예측기의 성능을 보여주고, oob샘플은 앙상블 학습에서 사용된 개별예측기의
성능을 나타내 준다는 것이 차이점 같다.

그다음 배깅/페이스팅 방법을 적용한 결정트리의 앙상블을 최적화 한 모델을 랜덤 포레스트라 한다.
이는 트리의 노드를 분할할 때, 전체 특성중에서 최선의 특성을 찾는 대신 무작위성을 더 주입함으로써, 트리를 더욱 다양하게 만들고, 평향을 손해보는대신 분산을 낮추는 역할을 한다.
또한 엑스트라 트리라는 것도 존재하는데 이는 특성뿐만 아니라 특성 임곗값 또한 무작위로 선택하기 때문에 극단적으로 무작위한 트리의 랜덤 포레스트라 할 수 있다.

부스팅은 성능이 약한 학습기를 여러개 연결하여 강한 성능의 학습기를 만드는 앙상블 기법이다. 이것은 순차적으로 이전 학습기의 결과를 바탕으로 성능을 조금씩 높혀가는 방식으로
학습을 진행해 나간다.

그중 에이다부스트는 좀 더 나은 예측기를 생성하기 위해 잘 못 적용된 가중치를 조정하여 새로운 예측기를 추가하는 앙상블 기법을 말하고, 이전에 과소적합했던 훈련 샘플들에 대한
가중치를 더 높히는 방식으로 새로운 모델을 생성한다.

그레이디언트 부스팅은 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 대해 새로운 예측기를 학습시킨다. 그리고 이 잔여 오차를 줄이는 방식으로 모델을 학습시키는데,
이때 경사 하강법을 사용하여 최적화 한다.

마지막으로 스태킹이 있는데 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를사용하는 대신 취합하는 모델을 훈련시키는 것이 키 아이디어이다.
하지만 현재 우리가 배우는 사이킷런은 스태킹을 직접 지원하지 않기 때문에 구현을 해야한다.
