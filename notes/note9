12주차 정리노트
10조 - 최정환, 김성영

이번 수업에서는 딥러닝 인공 신경망에 대해서 배웠다.
딥러닝에는 인공 뉴런을 사용한 논리 연산을 통해 학습을 하는데, 인공 뉴런이란 생물학적 뉴런에 착안한 매우 단순한 신경망 모델이며,
하나 이상의 이진 입력과 이진 출력 하나를 가진다.
그 중 퍼셉트론은 가장 간단한 인공 신경망 구조로 TLU 또는 LTU라 불리는 인공 뉴런을 활용한다.
퍼셉트론의 모든 입력은가중치와 연결되며, 입력값과 가중치를 곱한 값을의 합에 계단함수를 적용한다.

다층 퍼셉트론은 퍼셉트론을 여러 개 쌓아올린 인공 신경망으로, 입력층 하나와, 은닉층이라 불리는 하나 이상의 TLU층, 그리고 출력층으로 구성되어있다.
모든 층은 편향을 포함하고, 다음 층과 완전히 연결되어 있다.
이 다층퍼셉트론은 층이 많을수록 훈련시키는 과정이 점점 더 어려워 지는데, 이를위해 역전파 훈련 알고리즘을 이용한다.
역전파 훈련 알고리즘이란 첫번째로 각 훈련샘플에 대해 먼저 예측을 만든 후 오차를 측정하고,
두번빼로 역방향으로 각 층을 거치면서 각 출력연결이 오차에 기여한 정도를 측정한다. 마지막으로 오차가 감소하도록 모든 가중치를 조정한다.
그리고 활성화 함수로는 계단함수 대신 렐루, 로지스틱, 하이퍼볼릭 탄젠트 함수 등등을 사용하는데,
이는 비선형 활성화 함수를 충분히 많은 층에서 사용하면 매우 강력한 모델을 학습 가능하기 때문이다.

강의에서 케라스 시퀀셜API를 활용한 회귀 예측 모델을 학습하는 예제가 나왔는데 이때, 캘리포니아 데이터셋과 관련된 주의할 점에 잡음이 많기 때문에,
과대적합에 주의해서 뉴런수가 적은 하나의 은닉층만 사용했다고 적혀있다.
왜냐하면 은닉층과 뉴런이 많이 사용될 수록 가중치 파라미터의 수가 증가하여 과대적합 위험도가 커지기 때문이다.
하지만 뉴런의 수와 은닉층의 수가 너무적으면 과소적합의 위험이 너무 커지지 않을까 생각을 했었는데, 
때문에 인공 신경망같은 경우 어떻게 해야 최적으로 튜닝을 할 수 있을지 의문이 생겼다. 왜냐하면 너무나 경우의 수가 많기 때문이다.

이에 대해 먼저 콜백 함수를 사용해서 최상의 검증 세트 파라미터를 저장 가능한데, 이를 통해 최적의 에포크 수를 찾을 수 있다.
그 다음으로 신경망 하이퍼파라미터 튜닝은 그리드탐색, 랜덤탐색등을 활용해 케라스 모델을 사이킷런의 추정기처럼 보여주어야하기 때문에,
케라스 모델 생성함수를 입력해 주어야 그리드탐색, 랜덤탐색을 활용할 수 있다.
더 좋은 기법은 진화알고리즘, AutoML, kopt, spearmint등과 같은 다양한 라이브러리가 있다.

하지만 이것들을 활용하고도 결국 최적의 파라미터를 찾기 힘들 수 있는데 왜냐하면 은닉층 개수, 은닉층의 뉴런 수, 학습률, 옵티마이저, 배치크기, 활성화함수, 반복횟수등
너무나 많은 변수가 존재하기 때문이다. 그러므로 파라미터를 조정하는데 가장 중요한 것은 파라미터를 조정하는데 필요한 노하우와 경험을 통한 세부적인 조정이 제일
중요한 것 같다.
